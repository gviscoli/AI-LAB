# Single Neuron

## ReLU Function

The ReLU(x) function embodies the ReLU (Rectified Linear Unit) activation function. 

It returns x if x is positive, and zero otherwise. 

This function is key for introducing non-linearity, enabling neural networks to tackle complex problems.

## Neuron Simulation

![AI is fun!](/Assets/Images/SingleNode.png "Single Node Neuron")