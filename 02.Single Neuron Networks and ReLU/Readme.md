# Single Neuron

## ReLU Function

The ReLU(x) function embodies the ReLU (Rectified Linear Unit) activation function. 

It returns x if x is positive, and zero otherwise. 

This function is key for introducing non-linearity, enabling neural networks to tackle complex problems.

## Neuron Simulation

![AI is fun!](/Assets/Images/SingleNode.png "Single Node Neuron")

<b>Weights and Inputs</b>: Here, each input is multiplied by its corresponding weight. The weights are adjusted during training to refine the networkâ€™s accuracy

<b>Bias</b>: Adding the bias to the sum of weighted inputs provides an extra layer of flexibility, allowing the neuron to fine-tune its output.

<b>ReLU Activation</b>: The output from the linear calculation is then fed through the ReLU function. This is where non-linearity comes into play, empowering the network with the ability to learn and model intricate patterns.